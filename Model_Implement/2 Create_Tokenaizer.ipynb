{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7503e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    PreTrainedTokenizerFast,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40086125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Train Thai tokenizer with SentencePiece (no ## prefix)\n",
    "import sentencepiece as spm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, processors\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6eef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî® Training Thai tokenizer with SentencePiece...\")\n",
    "\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ corpus ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡πà‡∏≠‡∏ô\n",
    "if not os.path.exists('thai_corpus.txt'):\n",
    "    print(\"üì¶ Loading Thai corpus...\")\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # ‡πÇ‡∏´‡∏•‡∏î Thai handwriting dataset\n",
    "    ds = load_dataset(\"iapp/thai_handwriting_dataset\")[\"train\"]\n",
    "    \n",
    "    # ‡∏™‡∏Å‡∏±‡∏î texts\n",
    "    all_texts = [item['text'] for item in ds if len(item['text']) > 10]\n",
    "    \n",
    "    print(f\"‚úÖ Collected {len(all_texts)} texts\")\n",
    "    \n",
    "    # Save\n",
    "    with open('thai_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_texts))\n",
    "    \n",
    "    print(\"‚úÖ Corpus saved to thai_corpus.txt\")\n",
    "else:\n",
    "    print(\"‚úÖ Found existing thai_corpus.txt\")\n",
    "\n",
    "# Train SentencePiece model\n",
    "print(\"\\nüî® Training SentencePiece model...\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='thai_corpus.txt',\n",
    "    model_prefix='thai_sp_30000',\n",
    "    vocab_size=50000,\n",
    "    character_coverage=0.9995,\n",
    "    model_type='unigram',  # ‡∏´‡∏£‡∏∑‡∏≠ 'bpe'\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    pad_piece='[PAD]',\n",
    "    unk_piece='[UNK]',\n",
    "    bos_piece='[CLS]',\n",
    "    eos_piece='[SEP]',\n",
    "    user_defined_symbols=['[MASK]'],\n",
    "    normalization_rule_name='identity', # ‡πÑ‡∏°‡πà‡πÅ‡∏õ‡∏•‡∏á case\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SentencePiece model trained\")\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('thai_sp_30000.model')\n",
    "\n",
    "print(f\"üìä Vocabulary size: {sp.vocab_size()}\")\n",
    "\n",
    "# Test\n",
    "test_text = \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\n",
    "tokens = sp.encode_as_pieces(test_text)\n",
    "ids = sp.encode_as_ids(test_text)\n",
    "decoded = sp.decode_pieces(tokens)\n",
    "\n",
    "print(f\"\\nüìù Test: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"IDs: {ids}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# ‚≠ê ‡πÑ‡∏°‡πà‡∏°‡∏µ ## prefix ‡πÅ‡∏•‡πâ‡∏ß!\n",
    "print(f\"\\n‚úÖ No '##' prefix in tokens!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Load your custom tokenizer correctly\n",
    "import sentencepiece as spm\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Define class again (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô memory)\n",
    "class SimpleSPTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"Simple wrapper for SentencePiece\"\"\"\n",
    "    \n",
    "    def __init__(self, sp_model_path, **kwargs):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(sp_model_path)\n",
    "        \n",
    "        super().__init__(\n",
    "            unk_token=\"<unk>\",\n",
    "            bos_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.sp.vocab_size()\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return {self.sp.id_to_piece(i): i for i in range(self.sp.vocab_size())}\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        return self.sp.encode_as_pieces(text)\n",
    "    \n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.sp.piece_to_id(token)\n",
    "    \n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.sp.id_to_piece(index)\n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return self.sp.decode_pieces(tokens)\n",
    "    \n",
    "    def save_vocabulary(self, save_directory, filename_prefix=None):\n",
    "        import shutil\n",
    "        import os\n",
    "        \n",
    "        if not os.path.isdir(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        out_file = os.path.join(\n",
    "            save_directory, \n",
    "            (filename_prefix + \"-\" if filename_prefix else \"\") + \"spm.model\"\n",
    "        )\n",
    "        \n",
    "        shutil.copy('thai_sp_30000.model', out_file)\n",
    "        return (out_file,)\n",
    "\n",
    "# ‚≠ê ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏î‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡πÉ‡∏´‡∏°‡πà\n",
    "your_tokenizer = SimpleSPTokenizer('thai_sp_30000.model')\n",
    "\n",
    "print(\"‚úÖ Your tokenizer loaded\")\n",
    "\n",
    "# Test\n",
    "test_text = \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\n",
    "your_tokens = your_tokenizer.tokenize(test_text)\n",
    "your_encoded = your_tokenizer.encode(test_text, add_special_tokens=False)\n",
    "your_decoded = your_tokenizer.decode(your_encoded)\n",
    "\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"Tokens: {your_tokens}\")\n",
    "print(f\"Decoded: '{your_decoded}'\")\n",
    "print(f\"Match: {your_decoded.strip() == test_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your tokenizer\n",
    "your_tokens = ['‚ñÅ', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏Ñ‡∏£‡∏±‡∏ö', '‚ñÅ', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢']\n",
    "\n",
    "# WangchanBERTa\n",
    "wc_tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "wc_tokens = wc_tokenizer.tokenize(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\")\n",
    "\n",
    "print(\"Your tokenizer:\", your_tokens)\n",
    "print(\"WangchanBERTa:\", wc_tokens)\n",
    "print(f\"\\nCleaner: WangchanBERTa\" if len(wc_tokens) < len(your_tokens) else \"Yours\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
